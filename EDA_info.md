
## üî¨ An√°lisis 1: Integridad y Dise√±o Estructural del Dataset

Este an√°lisis confirma la alta calidad y la estructura uniforme del *dataset*, lo que simplifica el *pipeline* de preprocesamiento y permite el dise√±o directo de la arquitectura U-Net.

### 1.1 Resumen del Conteo y Formato de Datos

| M√©trica | Valor | Conclusi√≥n e Impacto |
| :--- | :--- | :--- |
| **Total de Im√°genes (Train)** | 2133 | Conjunto de datos de tama√±o moderado. |
| **Total de M√°scaras (Train)** | 2133 | **Integridad Perfecta.** Coincidencia $1:1$ de pares. |
| **Tama√±o Fijo** | $800 \times 800$ | **CR√çTICO.** Elimina la necesidad de *Data Augmentation* de escalado o *padding* variable. La U-Net se dise√±a para esta dimensi√≥n exacta. |
| **M√°scaras Vac√≠as** | 0 ($0.00\%$) | **Alta Calidad.** Todos los *samples* de entrenamiento contienen el objeto de inter√©s. |
| **Decodificaci√≥n** | Exitosa (Canal Alfa / Grayscale) | Se confirma la accesibilidad y validez del *ground truth* binario. |

### 1.2 Implicaciones en el Dise√±o de la U-Net

El formato uniforme $800 \times 800$ permite una arquitectura U-Net con una **entrada fija**, asegurando que todas las operaciones de *pooling* y convoluci√≥n sean consistentes. Esto contribuye a la **estabilidad del entrenamiento** desde la primera √©poca.

| Decisi√≥n de Dise√±o | Justificaci√≥n (Basada en $800 \times 800$) |
| :--- | :--- |
| **Arquitectura** | **U-Net de entrada $800 \times 800$**. El *Encoder* deber√° ser lo suficientemente profundo para capturar el contexto global de la escena con un tama√±o de *feature map* manejable. |
| **Pre-procesamiento** | **Solo Normalizaci√≥n y Augmentation**. No se requiere un paso de *resize* forzado, optimizando el tiempo de carga de datos. |

---


## üé® An√°lisis 2: Estad√≠sticas de Color y Normalizaci√≥n

Este an√°lisis cuantifica la distribuci√≥n de intensidad de p√≠xeles, revelando un balance de color y variabilidad significativos que deben ser compensados mediante la normalizaci√≥n.

### 2.1 Resultados Cuantitativos (Media y Desviaci√≥n Est√°ndar)

| M√©trica | Canal Rojo (R) | Canal Verde (G) | Canal Azul (B) |
| :--- | :--- | :--- | :--- |
| **Media ($\mu$)** | **0.515** | **0.484** | **0.460** |
| **Desviaci√≥n Est√°ndar ($\sigma$)** | **0.314** | **0.303** | **0.303** |

### 2.2 Interpretaci√≥n del Histograma de Intensidad

  * **Distribuci√≥n Multimodal:** El histograma muestra claramente **picos en ambos extremos** (cerca de 0.0 y 1.0) para todos los canales, con un fuerte sesgo en los valores bajos (sombras) y altos (luces/fondos blancos). Esto indica una **alta variabilidad de iluminaci√≥n y contraste** en el *dataset*.
  * **Balance de Color:** La media de los canales **R (0.515) y G (0.484)** es ligeramente **superior** a la media del canal **B (0.460)**, sugiriendo una ligera dominante de tonos c√°lidos o una mayor presencia de piel/fondos rojizos/verdosos.
  * **Contraste Elevado:** Los valores de la **Desviaci√≥n Est√°ndar ($\sigma \approx 0.30$)** son relativamente altos. Para datos normalizados en $[0, 1]$, un $\sigma$ de $0.3$ a $0.35$ indica una **distribuci√≥n amplia** de intensidades (alto contraste y alta variabilidad de iluminaci√≥n).

### 2.3 Justificaci√≥n del Pre-procesamiento: Normalizaci√≥n Z-score

El objetivo del pre-procesamiento es transformar los datos de entrada para que la red neuronal pueda aprender de forma m√°s eficiente.

| Decisi√≥n de Pre-procesamiento | Justificaci√≥n y Ventaja |
| :--- | :--- |
| **Normalizaci√≥n Z-score** | Se implementar√° la f√≥rmula $\text{Imagen}_{\text{norm}} = (\text{Imagen} - \mu) / \sigma$. |
| **Impacto en el Modelo** | Esta t√©cnica **centra los datos** alrededor de cero ($\mu$) y los **escala** a una desviaci√≥n est√°ndar unitaria ($\sigma$), previniendo que los valores de intensidad m√°s grandes (ej., p√≠xeles blancos) dominen la funci√≥n de activaci√≥n de las capas iniciales de la U-Net. Esto es **cr√≠tico** para la convergencia r√°pida y estable del optimizador. |
| **Valores a Usar** | **Media:** $[0.515, 0.484, 0.460]$ <br> **Desv. Est.:** $[0.314, 0.303, 0.303]$ |

-----


## üßç‚Äç‚ôÇÔ∏è An√°lisis 3: Desequilibrio de P√≠xeles y Funci√≥n de P√©rdida

Este an√°lisis cuantifica la proporci√≥n de la **Clase Positiva (Persona)** y justifica la necesidad de utilizar una funci√≥n de p√©rdida robusta contra el desequilibrio de clases (p√≠xeles).

### 3.1 Resultados Cuantitativos del Desequilibrio

| M√©trica | Valor | Conclusi√≥n e Impacto |
| :--- | :--- | :--- |
| **P√≠xeles Positivos ($\mu$ Promedio)** | **$39.90\%$** | La clase de inter√©s ("Persona") no es tan minoritaria como se podr√≠a esperar en segmentaci√≥n, pero sigue siendo la **clase minoritaria** frente al $60.10\%$ de "Fondo". |
| **Desviaci√≥n Est√°ndar** | $21.16\%$ | **Alta Variabilidad.** La segmentaci√≥n debe ser efectiva tanto para personas grandes (hasta $100\%$) como para personas peque√±as (desde $1.04\%$). |
| **Rango de √Årea** | $1.04\%$ a $100.00\%$ | **Reto de Escala.** El modelo debe aprender a segmentar objetos que var√≠an dram√°ticamente en tama√±o (desde peque√±os detalles hasta cuerpos completos). |

### 3.2 Interpretaci√≥n del Histograma


* **Distribuci√≥n Ancha:** El histograma es amplio, con una dispersi√≥n significativa de la media. Esto confirma la **alta variabilidad en la escala y pose** de las personas. La red debe ser robusta a la escala (lo cual el U-Net ya maneja gracias a su estructura piramidal).
* **Ausencia de Desequilibrio Extremo:** El promedio de casi $40\%$ es mucho m√°s alto que el $1-5\%$ t√≠pico en segmentaci√≥n m√©dica. Esto aten√∫a el riesgo de que el modelo solo aprenda "Fondo", pero el riesgo sigue siendo significativo.

### 3.3 Justificaci√≥n de la Funci√≥n de P√©rdida y M√©trica

Dada la variabilidad en el √°rea y el hecho de que el "Fondo" sigue siendo la clase mayoritaria ($60\%$), la funci√≥n de p√©rdida debe priorizar la precisi√≥n en la clase minoritaria ("Persona").

| Decisi√≥n de Modelado | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **Funci√≥n de P√©rdida** | **Dice Loss o Focal Loss (Recomendado: Dice Loss).** La **Cross-Entropy (BCE)** tradicional penalizar√≠a en exceso a los p√≠xeles de fondo, resultando en un modelo que predice "Fondo" con demasiada facilidad. La Dice Loss se enfoca en la superposici√≥n (Intersection over Union, IoU), que es crucial para la segmentaci√≥n y maneja mejor este nivel de desequilibrio. |
| **M√©trica de Evaluaci√≥n** | **Dice Coefficient (IoU).** Ser√° la m√©trica principal para la validaci√≥n y el *Test Set*. A diferencia de la precisi√≥n simple, el IoU refleja con precisi√≥n la calidad de la segmentaci√≥n en la clase minoritaria, que es el objetivo del proyecto. |

---


## üëÅÔ∏è An√°lisis 4: Visualizaci√≥n de Bordes y Arquitectura

La inspecci√≥n visual de los pares Imagen-M√°scara y su *Overlay* valida la calidad del *ground truth* y proporciona la justificaci√≥n cr√≠tica para la selecci√≥n de una arquitectura que priorice la **precisi√≥n espacial** y la **captura de detalles finos**.

### 4.1 Observaciones Clave del *Overlay*


* **Alineaci√≥n Precisa:** Los *overlays* (Columna 3) muestran una alineaci√≥n excelente entre la m√°scara y el contorno real de la persona. Esto confirma la **alta calidad del etiquetado**, minimizando el ruido en las etiquetas que podr√≠a confundir al modelo.
* **Contornos Complejos:** Las m√°scaras deben capturar bordes finos e irregulares, incluyendo:
    * **Contornos Org√°nicos:** Cabello, manos, dedos (Ej. `1286.png`).
    * **Contornos de Ropa:** Pliegues del vestido o tela suelta (Ej. `1870.png`).
    * **Espacios Internos:** El espacio entre las piernas o brazos (Ej. `387.png`).
* **Variabilidad de Pose y Escala:** Las im√°genes capturan poses muy variadas (sentado, agachado, de pie, vista lateral/trasera), lo que demanda que el modelo aprenda caracter√≠sticas invariantes a la pose.

### 4.2 Justificaci√≥n de la Arquitectura U-Net

El reto principal de este *dataset* es la **precisi√≥n de la segmentaci√≥n a nivel de p√≠xel** en los bordes org√°nicos y complejos.

| Decisi√≥n de Modelado | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **Arquitectura Principal** | **U-Net.** El modelo de segmentaci√≥n *debe* ser una U-Net (o variante como FPN/LinkNet) debido a la **necesidad cr√≠tica de capturar el detalle del borde**. |
| **Importancia de las *Skip Connections*** | **Esencial.** El *Encoder* (que reduce la resoluci√≥n para capturar el **contexto** global) perder√≠a los detalles finos. Las *skip connections* transfieren la **informaci√≥n de alta resoluci√≥n** (los bordes) directamente desde el *Encoder* al *Decoder*. Esto asegura que la reconstrucci√≥n final de la m√°scara sea n√≠tida y precisa, en lugar de borrosa o suave. |
| **Generalizaci√≥n** | El modelo debe ser capaz de generalizar los bordes en condiciones variadas de fondo, iluminaci√≥n y color (lo cual ser√° reforzado por el *Data Augmentation*). |


---

## üé® An√°lisis 5: Brillo, Contraste y Saturaci√≥n (HSV)

Este an√°lisis cuantifica la variabilidad del color y la iluminaci√≥n, demostrando la necesidad de aplicar **Data Augmentation** para hacer que la U-Net sea robusta a las condiciones de iluminaci√≥n del mundo real.

### 5.1 Resultados Cuantitativos del An√°lisis HSV

| M√©trica | Valor Promedio ($\mu$) | Conclusi√≥n |
| :--- | :--- | :--- |
| **Brillo** (Canal V) | **$140.36$** (de 255) | **Iluminaci√≥n Neutra a Brillante.** La media est√° ligeramente por encima del punto medio ($128$), indicando que el *dataset* tiende a estar bien o sobre expuesto. |
| **Contraste** ($\sigma$ de V) | **$60.39$** (de 255) | **Contraste Moderado.** La distribuci√≥n es claramente normal (forma de campana), lo que significa que el contraste es consistente, pero no extremo. |
| **Saturaci√≥n** (Canal S) | **$67.47$** (de 255) | **Saturaci√≥n Baja.** La media est√° bien por debajo del punto medio, y el histograma est√° sesgado hacia valores bajos (cercanos a 0). Esto sugiere que la mayor√≠a de las im√°genes tienen **colores apagados o gris√°ceos** (ambientes interiores o condiciones de luz difusa). |

### 5.2 Justificaci√≥n de la Data Augmentation de Color

El objetivo es evitar que el modelo se sobreajuste a la iluminaci√≥n y saturaci√≥n promedio del *dataset* (es decir, el modelo no debe fallar solo porque la foto es m√°s brillante o los colores son m√°s vivos).

| Componente HSV | Decisi√≥n de Augmentation | Par√°metros Sugeridos (Ej. en rango 0.0 a 1.0) |
| :--- | :--- | :--- |
| **Brillo (Valor)** | **Jitter Sim√©trico.** Dado que la media es $140.36$, la variaci√≥n debe ser en ambos sentidos para simular d√≠as oscuros y d√≠as muy soleados. | **(0.7, 1.3):** Multiplicador de $0.7$ (oscuro) a $1.3$ (brillante) sobre la imagen normalizada. |
| **Contraste ($\sigma$ de V)** | **Jitter Moderado.** La distribuci√≥n es centrada. Se debe aplicar variaci√≥n para simular fotos "planas" y fotos con luces y sombras duras. | **(0.75, 1.25):** Variaci√≥n de $\pm 25\%$ en contraste. |
| **Saturaci√≥n (Canal S)** | **Jitter Agresivo.** La baja media ($67.47$) y el sesgo hacia 0 obligan a **introducir colores m√°s vivos** de forma artificial. | **(0.5, 1.5):** Variaci√≥n desde $50\%$ menos (casi blanco y negro) hasta $50\%$ m√°s (colores muy vivos). Esto es **cr√≠tico** para la generalizaci√≥n. |

---

¬°Estos resultados son sorprendentes e importantes! El **Paso 6: Calidad de M√°scaras** revela que casi la totalidad del *dataset* est√° marcado como potencialmente ruidoso ($2115$ de $2133$). Sin embargo, la clave est√° en el **tipo de ruido**.

Aqu√≠ tienes el an√°lisis formal y la justificaci√≥n para tu informe en formato Markdown:

---

## üßº An√°lisis 6: Calidad de M√°scaras y Estrategia de Post-Procesamiento

El an√°lisis de calidad detect√≥ ruido en la abrumadora mayor√≠a de las m√°scaras, lo que requiere una decisi√≥n informada sobre la estrategia de post-procesamiento del modelo.

### 6.1 Resultados Cuantitativos del An√°lisis de Ruido

| M√©trica | Valor | Conclusi√≥n e Implicaci√≥n |
| :--- | :--- | :--- |
| **Total de M√°scaras** | 2133 | |
| **M√°scaras Ruidosas Detectadas** | **2115** ($99.15\%$) | **ALERTA.** Casi todas las m√°scaras contienen ruido seg√∫n los criterios de detecci√≥n (agujeros O componentes > 1). |
| **Componentes Separados** (Ejemplo) | **1** (Caso Duda: `990.png`) | La gran mayor√≠a de las m√°scaras tienen **1 componente principal** (la persona), lo que es correcto. El ruido es **principalmente por agujeros**. |
| **Agujeros Detectados** (Ejemplo) | **True** (Caso Duda: `990.png`) | La operaci√≥n de cierre detect√≥ que casi todas las m√°scaras contienen **peque√±os agujeros internos** o interrupciones. |

### 6.2 Interpretaci√≥n de la Detecci√≥n de Ruido

La alta tasa de detecci√≥n de "ruido" no significa necesariamente que las etiquetas est√©n rotas, sino que **los contornos de la persona son altamente org√°nicos e irregulares**. Los agujeros detectados son probablemente:

1.  Espacios entre los dedos, brazos, o el cabello.
2.  Agujeros intencionales de etiquetado (ej., un anillo o un *piercing* en la m√°scara).

Dado que el $99.15\%$ de las m√°scaras tienen este "ruido", esto debe considerarse una **caracter√≠stica intr√≠nseca** de la clase "Persona" y no un error a limpiar previamente.

### 6.3 Justificaci√≥n de la Estrategia de Post-Procesamiento

La detecci√≥n masiva de agujeros implica un desaf√≠o: el modelo debe aprender a llenar o ignorar estos peque√±os vac√≠os.

| Decisi√≥n de Modelado | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **Post-Procesamiento** | **Obligatorio pero Controlado.** Dado que la tasa de detecci√≥n es $99.15\%$ (mucho mayor al umbral del $2\%$), el post-procesamiento es esencial, pero su objetivo debe ser solo **suavizar** la predicci√≥n. |
| **T√©cnica Recomendada** | Aplicar una **operaci√≥n morfol√≥gica de CIERRE** (`cv2.MORPH_CLOSE` con un *kernel* peque√±o, ej. $3 \times 3$ o $5 \times 5$) a la **predicci√≥n binaria final** del modelo. |
| **Impacto en el Modelo** | Esto sirve como una capa de correcci√≥n final para **rellenar los peque√±os fallos de segmentaci√≥n** (agujeros) y **eliminar el ruido aislado** (*salt and pepper noise*) que la U-Net podr√≠a no haber manejado perfectamente en los bordes. |

---


## üìâ An√°lisis 7: Espectro de Magnitud Promedio (FFT)

El an√°lisis espectral de la transformada de Fourier revela d√≥nde se concentra la "energ√≠a" de las im√°genes, lo que nos permite inferir la complejidad de las texturas y los detalles finos.

### 7.1 Interpretaci√≥n del Espectro


* **Concentraci√≥n Central Extrema:** La energ√≠a est√° casi en su totalidad concentrada en el **punto central** del espectro. Este centro representa las **bajas frecuencias** (formas grandes, √°reas de color uniforme, fondos suaves, etc.).
* **Decaimiento R√°pido:** La intensidad de la frecuencia decae muy r√°pidamente a medida que nos alejamos del centro hacia los bordes. Los **bordes** del espectro representan las **altas frecuencias** (detalles finos, texturas, ruido, bordes agudos).
* **Conclusi√≥n:** El *dataset* est√° dominado por **bajas frecuencias**. Las im√°genes, en promedio, **carecen de texturas finas complejas** o tienen fondos muy suaves.

### 7.2 Justificaci√≥n de Data Augmentation (Textura)

La U-Net segmentar√° bien las formas grandes, pero podr√≠a **sobreajustarse a la suavidad** de las im√°genes de entrenamiento y **fallar al encontrar la persona en entornos ruidosos o texturizados** (ej., fotos con grano o ambientes urbanos complejos).

| Decisi√≥n de Augmentation | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **Adici√≥n de Ruido** | **Ruido Gaussiano o Ruido de Sal y Pimienta.** Se debe inyectar ruido artificialmente en los datos de entrenamiento. |
| **Impacto en el Modelo** | Esto fuerza al modelo a aprender que la segmentaci√≥n (el objeto "Persona") es v√°lida **incluso en presencia de variaciones de alta frecuencia** (ruido). El modelo se vuelve **robusto a la textura** y es menos probable que confunda el ruido de alta frecuencia del entorno real con un fallo en la segmentaci√≥n del borde. |
| **Augmentation Geom√©trica** | **Rotaciones y Shearing.** La l√≠nea cruzada (artefacto de las im√°genes rectangulares) es visible, justificando tambi√©n la necesidad de rotar para hacer el modelo invariante a la orientaci√≥n. |


---

## üìä An√°lisis 8: Ruido/Textura Local 

Este an√°lisis cuantifica la granularidad o textura fina de las im√°genes midiendo la desviaci√≥n est√°ndar promedio en parches locales. Confirma num√©ricamente las conclusiones extra√≠das del an√°lisis FFT.

### 8.1 Resultados Cuantitativos del An√°lisis de Textura

| M√©trica | Valor | Conclusi√≥n e Implicaci√≥n |
| :--- | :--- | :--- |
| **Media de $\sigma_n$ (Textura Local)** | **$10.82$** (de 255) | **Valor Muy Bajo.** Una media de 10.82 en una escala de 0-255 indica que las im√°genes son, en promedio, **extremadamente suaves** y carecen de textura fina o ruido de sensor. |

### 8.2 Interpretaci√≥n del Histograma


* **Pico en Valores Bajos:** El histograma muestra una fuerte concentraci√≥n de im√°genes con un $\sigma_n$ entre 5 y 15. Esto confirma que la gran mayor√≠a del *dataset* est√° compuesto por im√°genes "limpias", con fondos desenfocados (*bokeh*) o superficies de piel/ropa suaves.
* **Cola Derecha (Skew):** Aunque la media es baja, la cola se extiende hasta $\approx 40$. Esto indica que existe un subconjunto minoritario de im√°genes que s√≠ contienen textura o ruido, pero no son la norma.

### 8.3 Justificaci√≥n de Data Augmentation (Ruido)

Este an√°lisis, combinado con el FFT (Paso 7), presenta una justificaci√≥n s√≥lida para la **Augmentation de Ruido**.

| Decisi√≥n de Augmentation | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **A√±adir Ruido (Ej. `GaussianNoise`)** | **CR√çTICO.** El valor medio de $\sigma_n=10.82$ es muy bajo, lo que refuerza que el modelo se sobreajustar√° a im√°genes "limpias". El modelo podr√≠a confundir el grano de una foto real o la textura de una tela con un borde, resultando en una segmentaci√≥n ruidosa. |
| **Impacto en el Modelo** | Al a√±adir ruido artificial (ej. `GaussianNoise`) durante el entrenamiento, **forzamos a la U-Net a aprender la diferencia entre la "textura" (ruido) y la "forma" (bordes reales)**. Esto es vital para la generalizaci√≥n y la robustez del modelo en el *Test Set*. |

---

## üìê An√°lisis 9: Compacidad y Complejidad del Borde

Este an√°lisis mide la complejidad geom√©trica de las m√°scaras. La **compacidad** (calculada como $\frac{(\text{Per√≠metro})^2}{4\pi \times \text{√Årea}}$) cuantifica cu√°n irregular es un contorno. Un c√≠rculo perfecto tiene una compacidad de $1.0$; valores m√°s altos indican formas m√°s complejas.

### 9.1 Resultados Cuantitativos de Compacidad

| M√©trica | Valor | Conclusi√≥n e Implicaci√≥n |
| :--- | :--- | :--- |
| **Compacidad Media ($\mu$)** | **$2.83$** | **Valor Significativamente Alto.** Una media de 2.83 (muy lejos del 1.0 de un c√≠rculo o 1.27 de un cuadrado) confirma que las m√°scaras son **geom√©tricamente complejas**. |

### 9.2 Interpretaci√≥n del Histograma


* **Pico Desplazado:** El pico de la distribuci√≥n se encuentra alrededor de $2.0-2.5$, no de $1.0$. Esto indica que la "norma" en este *dataset* no son formas simples, sino contornos irregulares.
* **Fuerte Sesgo a la Derecha (Cola Larga):** El histograma tiene una cola larga que se extiende hasta valores $> 16.0$. Esto es causado por poses que maximizan el per√≠metro en relaci√≥n con el √°rea (ej. brazos extendidos, piernas separadas, cabello detallado), representando los casos de segmentaci√≥n m√°s dif√≠ciles.

### 9.3 Justificaci√≥n de la Arquitectura U-Net

Este an√°lisis refuerza la decisi√≥n de usar U-Net, tomada en el Paso 4.

| Decisi√≥n de Modelado | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **Arquitectura (U-Net)** | La alta media de compacidad ($2.83$) prueba cuantitativamente que el modelo debe ser capaz de **reconstruir formas irregulares**, no solo "manchas" (blobs). La U-Net es ideal para esto. |
| **Importancia de las *Skip Connections*** | **CR√çTICO.** La alta compacidad se debe a los **detalles de alta frecuencia** (bordes finos). Las *skip connections* son el mecanismo que permite a la U-Net transferir esta informaci√≥n espacial precisa desde el *encoder* al *decoder*, permitiendo la reconstrucci√≥n de bordes complejos. |
| **Data Augmentation (Geom√©trica)** | Para robustecer el modelo contra las formas m√°s extremas (la cola derecha), se justifica el uso de **Elastic Deformation** o **Grid Distortion**, ya que simulan variaciones org√°nicas en los contornos. |

---


## üó∫Ô∏è An√°lisis 10: An√°lisis de Posici√≥n (Distancia al Centro)

Este an√°lisis mide el **"sesgo de centrado"** del *dataset* calculando la distancia promedio (en p√≠xeles) desde el centroide de la persona hasta el centro exacto de la imagen ($400, 400$).

### 10.1 Resultados Cuantitativos del An√°lisis de Centrado

| M√©trica | Valor | Conclusi√≥n e Implicaci√≥n |
| :--- | :--- | :--- |
| **Distancia Media ($\mu$)** | **$119.09$ px** | **Sesgo Significativo.** El sujeto no est√° centrado. En promedio, el centro de la persona est√° a $\approx 119$ p√≠xeles del centro de la imagen. |
| **Desviaci√≥n Est√°ndar** | $71.81$ px | **Alta Variabilidad.** La posici√≥n del sujeto var√≠a mucho de una imagen a otra. |
| **Distancia M√°xima** | $446.90$ px | **Casos Extremos.** Existen sujetos cuyo centroide est√° a $\approx 447$ p√≠xeles del centro (muy cerca de las esquinas o bordes). |

### 10.2 Interpretaci√≥n del Histograma


* **Pico Desplazado:** El histograma no est√° centrado en 0. Muestra un pico claro alrededor de $80-120$ p√≠xeles, confirmando que la mayor√≠a de los sujetos est√°n notablemente descentrados.
* **Cola Larga:** La cola se extiende hasta $\approx 450$ p√≠xeles, lo que indica que el modelo debe ser capaz de segmentar personas que est√°n casi completamente en el borde de la imagen.

### 10.3 Justificaci√≥n de la Data Augmentation Geom√©trica

Estos resultados justifican la necesidad de hacer el modelo **invariante a la traslaci√≥n (posici√≥n)**. El modelo no puede asumir que la persona estar√° siempre en el centro.

| Decisi√≥n de Augmentation | Justificaci√≥n de los Resultados |
| :--- | :--- |
| **Traslaci√≥n Aleatoria** | **CR√çTICO.** Se deben aplicar **`Random Shifts`** (traslaciones) o **`RandomResizedCrop`** (que maneja tanto escala como traslaci√≥n). |
| **Impacto en el Modelo** | Esta augmentation simula sujetos en posiciones a√∫n m√°s variadas, forzando a la U-Net a aprender las **caracter√≠sticas de la "persona"** en lugar de aprender la **"posici√≥n de la persona"**. Esto es esencial para la generalizaci√≥n en el *Test Set*. |

